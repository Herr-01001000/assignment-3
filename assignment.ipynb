{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Data Management and Plotting\n",
    "\n",
    "## General Notes\n",
    "\n",
    "\n",
    "- The runtime of the solution will strongly depend on the Computer you are using, but even more on the code you write. If you often loop over each item in a pandas Series or DataFrame, the runtime explodes. In those cases you should look for a built-in pandas function that does the job.\n",
    "\n",
    "- Modified datasets should always be saved in a different directory than original data. Never ever overwrite an original data file! During this assignment, all generated output should be saved in a folder called `bld`. We sometimes ask you explicitly to save datasets as csv file. If you want to save more often, you are free to do so.\n",
    "\n",
    "- This assignment is not easy and depending on your previous experience it will take quite some time to complete it. However, we would never ask you to perform boring or repetitive tasks by hand. Whenever you find yourself typing long lists of variable names or you have several lines of code that look very similar, you can be sure that there is a better way. \n",
    "\n",
    "- Most of the tasks here have very short and elegant solutions. Our example solution has between 4 and 20 lines of code per task. You can use those numbers as a reference point for your own solution. If you have much longer solutions your are probably doing work that the pandas developers have already done for you. It can save you a lot of time to read up on pandas in [Wes McKinney's book](https://proquest.tech.safaribooksonline.de/9781491957653) or the [online documentation](http://pandas.pydata.org/pandas-docs/stable/). However, it is also important to note that short code is not a goal in itself. Readability is much more important. We will not deduct points if you have a long solution, but we will if your code is extremely hard to read. \n",
    "\n",
    "- Don't put your actual code in this notebook. Instead put it into `code/solution.py`. If you define many functions you can do so in different modules and just import them in `code/solution.py`.\n",
    "\n",
    "- All previous comments about naming files, tags and readable commit messages still apply.\n",
    "\n",
    "## Background\n",
    "\n",
    "In this assignment you will continue to work towards a replication of Cunha, Heckman and Schennach (CHS), Econometrica, 2010. This time you will clean data from the NLSY and transform it into the format needed later.\n",
    "\n",
    "Doing the complete data management of such a complicated project is not possible in one assignment (it often takes weeks or months). Therefore, you will only work with a small subset of the variables needed to replicate the paper. Moreover, we will save you some of the most painful steps by providing a pre-processed version of the dataset and csv files that will help you to harmonize variable names between panel waves. \n",
    "\n",
    "We will focus on the Behavior Problem Index that is used to measure non-cognitive skills. This index has the subscales antisocial behavior, anxiety, dependence, headstrong, hyperactive and peer problems. Here is an [overview](https://www.nlsinfo.org/content/cohorts/nlsy79-children/other-documentation/codebook-supplement/appendix-d-behavior-proble-0).\n",
    "\n",
    "The assignment repository contains a directory called `original_data`, with four files in it:\n",
    "\n",
    "- `BEHAVIOR_PROBLEMS_INDEX.dta`: Contains the main data you will work with. It is in wide format and the variable names are not informative. Moreover, the names do not contain the survey year in which the question was asked.\n",
    "- `bpi_variable_info.csv`: Contains information that will help you to decompose the main dataset into datasets for each year and to rename the variables such that the same questions get the same name across periods. In the real world you would have to generate this information yourself.\n",
    "- `BEHAVIOR_PROBLEMS_INDEX.cdb`: The codebook of the dataset. If you have any questions about the data, the answers are probably in the codebook.\n",
    "- `chs_data.dta`: The data file used in the original paper by Cunha Heckman and Schennach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Clone this repository, add a `.gitignore`-file as in exercise 2, and create branches with the github names of each group member. As last time, each of you will do work on his or her branch and occasionally merge the result into the master branch. This time you **don't** need a .tex file for the written solution.\n",
    "\n",
    "2. For a soft start, watch this [video](https://youtu.be/-_cLChzQzHM?t=101) from the official NLSY youtube channel. We only need the part from 1.40 to 2.42. Do you think it is a good idea to rename variables in the way described in the video? What are potential problems with this approach? (No need to write an answer, but discuss these questions in your group).\n",
    "\n",
    "3. Load the chs_data. Extract a list of ``childid``s that contains all values that variable takes in the chs_data, but no duplicates. Also, discard all observations in which `year` is not in \n",
    "```python\n",
    "    list(range(1986, 2011, 2))\n",
    "```\n",
    "\n",
    "4. Clean and transform the bpi dataset:\n",
    "    - Load the bpi dataset\n",
    "    - Use the list of `childid`s to only keep observations that are present in the chs_data. The column with child identifiers is called `C0000100` in the bpi dataset.\n",
    "    - Replace all negative numbers by ``pd.np.nan``\n",
    "    - Create a dictionary where the keys are the survey years (we only need the years 1986 - 2010) and the values are DataFrames with the bpi data of that year. In these DataFrames:\n",
    "        - All variables should have readable names from `bpi_variable_info.csv`\n",
    "        - You can discard all variables that are not present in `bpi_variable_info.csv`\n",
    "        - You should add a column called `year`, that indicates the survey year\n",
    "\n",
    "5. Use the results from the previous steps to generate a new bpi dataset in long format. Save this as a comma separated file in `bld/bpi_long.csv`.\n",
    "    \n",
    "6. Merge the long dataset with the chs dataset. Only keep observations that are present in the chs dataset. Use the columns `childid` and `year` as merge keys and give all variables that appear in both datasets (if there are any) sensible suffixes. Save this as a comma separated file in `bld/bpi_merged.csv`.\n",
    "\n",
    "7. Calculate scores for each subscale of the bpi by averaging the items of that subscale. The answers 'sometimes true' and 'often true', are counted as 1, 'not true' is counted as zero. Standardize the scores to mean 0 and variance 1 for each age group. Save the result as a comma separatade file in `bld/bpi_final.csv`.\n",
    "\n",
    "    **Note**: the dataset contains surprises such as categorical variables where you need numerical ones, variables that take values you did not expect or category labels that are similar but not identical across variables. Try to find good solutions for each of them.\n",
    "\n",
    "8. Make [regression plots](https://seaborn.pydata.org/generated/seaborn.regplot.html) for each subscale that show how your score relates to the same score in the chs data. Note that in the chs data missings are sometimes coded as -100. The names of their score relate to your names as follows: \n",
    "    ```python\n",
    "    {'antisocial': 'bpiA', 'anxiety': 'bpiB', 'headstrong': 'bpiC', 'hyperactive': 'bpiD', 'peer': 'bpiE'}\n",
    "    ```\n",
    "    The dependence scale has no counterpart in the chs data. \n",
    "\n",
    "    Since the variables are discrete, many datapoints are plotted on top of each other. Play around with the arguments of regplot until you think that your plot nicely displays the data. \n",
    "    \n",
    "    **Note:** If you did everything correctly you will get a strong but not perfect negative correlation in each case.\n",
    "\n",
    "9. The latent factor model behind the technology of skill formation implies that all items that make up the behavior problems index should be positively correlated, with especially high correlations between the items of one subscale. Make a [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) of the correlation matrix of the items from at least three subscales. Order the items such that items that belong to the same subscale are closer together. Choose a color scale in which it is easy to distinguish negative, positive and zero correlations without looking at numbers. \n",
    "\n",
    "10. Once you are satisfied with your solution, merge it into the master branch of your repository. Then add a tag called `exercise_3`  and push all branches (including the master branch **and the tag**) to the central server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
